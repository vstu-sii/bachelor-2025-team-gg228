# Итоги тестов

Дата: 2025-12-24

## Что проверялось

Цель — подтвердить, что проект собирается/стартует и базовые сценарии не блокируются синтаксическими/сборочными ошибками, а также зафиксировать текущее качество ML‑компонента (reranker) и риски.

### Набор проверок

| Область | Проверка | Результат | Комментарий |
|---|---|---:|---|
| Backend (FastAPI) | Синтаксис/импортируемость | ✅ | `python -m compileall backend/app -q` |
| Reranker (FastAPI) | Синтаксис/импортируемость | ✅ | `python -m compileall reranker/app -q` |
| Training scripts | Синтаксис | ✅ | `python -m compileall training -q` |
| Docker | Сборка образов | ✅ | `docker build -t poisk-backend ./backend`, `docker build -t poisk-reranker ./reranker`, `docker build -t poisk-frontend ./frontend` |
| Frontend (Vite) | Production build | ✅ | Запуск в контейнере node (локально `npm` отсутствовал) |
| ML‑качество | Pairwise accuracy (sample) | ⚠️ | `pairwise_accuracy=0.3850 (77/200)` для `cointegrated/rubert-tiny2` |

## Результаты и выводы

### 1) Сборка/CI‑уровень

- Python‑код компилируется (нет синтаксических ошибок).
- Vite production build проходит; итоговый бандл: `~186KB` JS gzip `~59KB`, CSS gzip `~3KB`.
- При `npm ci` выводится `2 moderate severity vulnerabilities` (нужен разбор/фиксация через обновление зависимостей).

### 2) ML‑качество reranker

- В `reranker` по умолчанию используется `cointegrated/rubert-tiny2` как base model.
- При оценке `training/evaluate_reranker.py` модель предупреждает о **неинициализированной head‑части** (классификатор), т.к. base‑чекпойнт не fine‑tuned под задачу.
- Итоговая точность на 200 первых примерах датасета `backend/data/training/rerank.jsonl`: `0.385` — это существенно ниже ожидаемого уровня и указывает, что rerank в текущем виде может **ухудшать** выдачу.

### 3) Наблюдаемость/метрики

- В проекте есть экспорт метрик и дашборды, но **нет зафиксированных целевых значений** (SLO/SLI) и нет автоматизированных нагрузочных/смоук‑прогонов, которые бы снимали p95/p99 и сравнивали baseline vs rerank.

## Сравнение с целевыми метриками (предлагаемые)

Текущие метрики качества/производительности формально не зафиксированы в требованиях; ниже — минимальные целевые ориентиры для MVP, чтобы было с чем сравнивать:

- **Reranker pairwise accuracy (offline)**: целевое `>= 0.60` на репрезентативной выборке (минимум 2k пар). Факт (sample 200): `0.385` ❌
- **Search p95 latency**: baseline `<= 1.5s`, rerank `<= 2.5s` на N=20 candidates. Факт: не измерено.
- **Ошибки API**: `5xx < 1%` за сессию демо. Факт: не измерено.

## Критические проблемы (P0/P1)

### P0 (блокеры качества)

- Reranker без обучения (случайная head‑часть) → высокий риск деградации качества результатов.
- Отсутствуют автоматизированные e2e/smoke‑тесты (хотя бы: поднять compose, загрузить документ, выполнить поиск, получить >=1 результат).

### P1 (важно для демо/стабильности)

- `npm` зависимости имеют умеренные уязвимости — нужны обновления/`npm audit` разбор.
- Docker‑compose для фронтенда использует dev‑server (`npm run dev`), а не production‑сборку; для демо это допустимо, но для релиза — нет.

## Приоритизация исправлений (кратко)

1. Обучить reranker на `training/data/rerank.jsonl` (или сгенерированном из БД) и примонтировать модель в сервис (`RERANKER_MODEL_PATH`).
2. Добавить smoke‑сценарий запуска (скрипт/док) и минимальный e2e‑прогон (docker compose + 1 запрос).
3. Обновить фронтенд зависимости и зафиксировать `npm audit` план.
