# Анализ целей

Дата: 2025-12-24

## 1) Цели MVP (по README/REPORT)

1. Дать пользователю поиск источников по тексту/файлу (PDF/DOCX).
2. Дать администратору скрытую панель управления (загрузка источников, пользователи, метрики).
3. Обеспечить наблюдаемость (Prometheus/Grafana, опционально Langfuse).
4. Подключить “свой ML‑компонент” как улучшение качества (reranker) и оставить baseline для сравнения.
5. Сделать воспроизводимый запуск (docker compose) и минимальный CI.

## 2) Статус достижения

| Цель | Статус | Комментарий |
|---|---:|---|
| Поиск по тексту/файлу | ✅ | Реализованы маршруты поиска, извлечение текста, чанкинг, retrieval. |
| Скрытая админ‑панель | ✅ | Роут `/panel`, доступ по роли `admin`, есть CRUD для документов/пользователей и просмотр метрик. |
| Observability | ✅ | Стек Prometheus/Grafana и опциональный Langfuse присутствуют в `docker-compose.yml`. |
| Reranker + baseline | ⚠️ | Интеграция есть, но качество reranker не подтверждено; по тесту `pairwise_accuracy=0.385` на base‑модели без обучения. |
| Воспроизводимый запуск + CI | ✅ | Есть `docker compose up --build` и workflow CI (compileall + build frontend + docker build). |

## 3) Проверка use-cases

### UC-1: Пользователь ищет источники по тексту

- Ожидаемо: ввод текста → выдача top‑результатов с цитатами/процентом совпадения.
- Статус: ✅ (функциональность реализована).
- Риск: без набора контрольных кейсов не доказано, что результаты релевантны на целевых документах.

### UC-2: Пользователь ищет источники по документу (PDF/DOCX)

- Ожидаемо: загрузка файла → извлечение текста → поиск → выдача источников.
- Статус: ✅ (парсинг PDF/DOCX предусмотрен).
- Риск: нет smoke/e2e‑прогона, который гарантирует стабильность цепочки “upload → parse → search”.

### UC-3: Администратор пополняет базу источников

- Ожидаемо: загрузка PDF/DOCX → индексация → документ доступен для поиска.
- Статус: ✅ (ingest реализован).
- Риск: нет метрик/алертов по ошибкам индексации и нет требований по времени индексации.

### UC-4: Администратор контролирует систему по метрикам

- Ожидаемо: видеть частоту запросов, задержки, ошибки, сравнение baseline vs rerank.
- Статус: ✅/⚠️ (метрики есть, но целевые пороги и регламент “как трактовать” не зафиксированы).

## 4) Валидация бизнес-метрик

### Что уже есть

- Технические метрики поиска (requests/duration) и инфраструктура для мониторинга.

### Чего не хватает, чтобы говорить о продуктовых целях

- События “пользователь открыл источник/перешёл к документу/скачал” (конверсия из выдачи в полезное действие).
- NSM и supporting metrics в явном виде (предложение зафиксировано в `docs/requirements-v3.md`).

## 5) Plan vs Fact (высокоуровнево)

- План по MVP‑функциям (поиск, панель, метрики, интеграции) — выполнен.
- План по “улучшенному поиску” — выполнен на уровне архитектуры/интеграции, но не выполнен на уровне качества (нет fine‑tuned модели и offline‑оценки на репрезентативной выборке).

## 6) Выводы и рекомендации

1. Зафиксировать минимальные целевые метрики качества/скорости (SLO) и добавить регулярный smoke‑прогон (см. DoD в `docs/requirements-v3.md`).
2. Обучить reranker и встроить offline‑оценку в процесс (хотя бы как ручной чек перед демо).
3. Добавить продуктовые события (клики по источникам) для привязки технической релевантности к пользе.
